# 5_ポッドスケジューリングの構成

<details>
<summary>リソース使用量を制限する</summary>
<div>

制限範囲(LimitRange)とリソースクォータ(Quota)を使用してアプリケーションのリソース消費を制限できるようになります。 

### リソースリクエストとポッドの制限の定義

- リソースリクエスト

スケジューリングに使用され、規定量未満の計算リソースではポッドを実行できないことを示します。スケジューラーは、ポッドのリクエストを満たす十分な計算リソースを持つノードの検出を試みます。 

- リソース制限

ポッドがノードのすべての計算リソースを使い尽くさないようにするために使用されます。ポッドを実行するノードは、Linux カーネルの cgroups 機能を設定してポッドのリソース制限を実行します。 


- 定義方法

1. **デプロイまたはデプロイ設定リソースのコンテナーごと**に定義する必要があります。リクエストと制限が定義されていない場合は、各コンテナーに resources: {} 行が表示されます。
```
...output omitted...
    spec:
      containers:                                    <- 各コンテナー
      - image: quay.io/redhattraining/hello-world-nginx:v1.0
        name: hello-world-nginx
        resources:                                   <- resource行
          requests:
            cpu: "10m"                               <- CPUとか
            memory: 20Mi                             <- メモリーをこのコンテナーは要求していることを示す。
          limits:
            cpu: "80m"                               <- CPUと
            memory: 100Mi                            <- メモリーの使用限界、このコンテナーはこれ以上は使わないことを指定
status: {}
```

2. oc editコマンドで

> oc set resources deployment hello-world-nginx **--requests** cpu=10m,memory=20Mi **--limits** cpu=80m,memory=100Mi

コマンドだと"="なんだなー、覚えづらい


### リクエスト、制限、実際の使用量の表示

- 計算使用情報を表示

ex) ノード：node1の**計算使用情報**を表示

> oc describe node node1
```
[user@demo ~]$ oc describe node node1
Name:               node1
Roles:              worker
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/instance-type=m4.xlarge
                    beta.kubernetes.io/os=linux
...output omitted...
Non-terminated Pods:                      (20 in total)
...  Name                CPU Requests  ...  Memory Requests  Memory Limits  AGE
...  ----                ------------  ...  ---------------  -------------  ---
...  tuned-vdwt4         10m (0%)      ...  50Mi (0%)        0 (0%)         8d
...  dns-default-2rpwf   110m (3%)     ...  70Mi (0%)        512Mi (3%)     8d
...  node-ca-6xwmn       10m (0%)      ...  10Mi (0%)        0 (0%)         8d
...output omitted...
  Resource                    Requests     Limits
  --------                    --------     ------
  cpu                         600m (17%)   0 (0%)
  memory                      1506Mi (9%)  512Mi (3%)
...output omitted...
```

- oc adm top コマンドは**実際の使用量**を表示
    - kubernetes.ioサフィックスが付いているものは内部的に使われ、弄れない。

> oc adm top nodes -l node-role.kubernetes.io/worker


### クォータの適用

OpenShift Container Platform は、**Kubernetesリソース (ポッド、サービス、ルートなど)の数** と**物理または仮想ハードウェアリソース (CPU、メモリー、ストレージ容量など)の数**を追跡および制限する**クォータ** を実行できます。

Kubernetes リソースの数にクォータを適用して、Etcd データベースの無制限の増大を阻止することで、OpenShift コントロールプレーンの安定性が向上します。

Kubernetes リソースにクォータを使用すると、制限のある他のソフトウェアリソース (サービスの IP アドレスなど) の消耗も防ぐことができます。 

クォータ属性は、プロジェクトにあるすべてのポッドのリソースリクエストまたはリソース制限を追跡できます。デフォルトでは、クォータ属性はリソースリクエストを追跡します。
リソース制限を追跡するときは、計算リソースの名前に limits.cpu のように**接頭辞 limits**を付けます。 

- YAML 構文を使用して定義された ResourceQuota リソース
```
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
spec:
  hard:
    services: "10"
    cpu: "1300m"
    memory: "1.5Gi"
```

- 上記のResourceQuoteのYAML構文を用いて、クォータを作成。

> oc create --save-config -f dev-quota.yml 

- クォータを作成する別の方法は、次のように oc create quota コマンドを使用。

> oc create quota dev-quota --hard services=10,cpu=1300,memory=1.5Gi

- oc describe quota コマンドを使用すると、プロジェクト内のすべての ResourceQuota リソースに対して設定された累積制限数が表示されます。

- oc delete コマンドによる削除

> oc delete resourcequota QUOTA


### 制限の範囲の適用

**LimitRangeリソース** は limit とも呼ばれ、**プロジェクト内で定義された 1 つのポッドまたは 1 つのコンテナーに対して、計算リソースのリクエストおよび制限のデフォルト値、最小値、最大値を定義します。**

制限範囲リソースでは、イメージ、イメージストリーム、または永続ボリューム要求によってリクエストされるストレージ容量のデフォルト値、最小値、最大値を定義することもできます。

**制限範囲(LimitRange)は 1 つのポッドの有効な範囲とデフォルト値を定義し、リソースクォータ(Quota)はプロジェクト内の全ポッドの最大合計値のみを定義することを考慮してください。**


- 制限範囲の作成は、以下のようなYAML または JSON リソースの定義ファイルを oc create コマンドに渡すことによって作成します。 

> dev-limits.yml
```
apiVersion: "v1"
kind: "LimitRange"
metadata:
  name: "dev-limits"
spec:
  limits:
    - type: "Pod"
      max:
        cpu: "500m"
        memory: "750Mi"
      min:
        cpu: "10m"
        memory: "5Mi"
    - type: "Container"
      default:
        cpu: "100m"
        memory: "100Mi"
      max:
        cpu: "500m"
        memory: "750Mi"
      min:
        cpu: "10m"
        memory: "5Mi"
```

> oc create --save-config -f dev-limits.yml

- プロジェクトに適用されている制限を表示

> oc describe limitrange dev-limits

- アクティブな制限範囲の削除

> oc delete limitrange dev-limits

</div>
</details>


### 覚えておきたいコマンド

- デプロイリソースファイルを作成し、~/DO280/labs/schedule-limit/hello-limit.yaml に保存します。

```
oc create deployment hello-limit --image quay.io/redhattraining/hello-world-nginx:v1.0 --dry-run -o yaml > ~/DO280/labs/schedule-limit/hello-limit.yaml
```

> memo

--dry-run -o yaml : 実行はされずにyaml形式で出力される


- 編集したリソースファイルを使用して、新しいアプリケーションを作成します。 
```
oc create --save-config -f ~/DO280/labs/schedule-limit/hello-limit.yaml
```

- アプリケーションに変更を適用
```
oc apply -f ~/DO280/labs/schedule-limit/hello-limit.yaml
```

oc applyで変更が適用され、自動で再作成？される

- TYPE=Warinigのイベントを確認できる
```
oc get events --field-selector type=Warning
```

- hello-limit アプリケーションに関連付けられているすべてのリソースを削除します。
```
oc delete all -l app=hello-limit
```

- schedule-limit プロジェクトを 3 つの CPU、1GB のメモリー、3 つの設定マップに制限する project-quota というクォータを作成
```
oc create quota project-quota --hard cpu="3",memory="1G",configmaps="3" -n schedule-limit(project名)
```

- project-quota クォータを削除
```
oc delete resourcequota project-quota
```


<details>
<summary>増加する要求に合わせてアプリケーションを拡張する</summary>
<div>
</div>
</details>

### 設定ワークロードでのポッドレプリカの指定

特定のデプロイまたはデプロイ設定のポッドレプリカの数をニーズに合わせて増減させることができます。

ReplicaSet および ReplicationController リソースにかかわらず、アプリケーションに必要なレプリカの数は**通常、デプロイまたはデプロイ設定(dc)リソースで定義されています。**

次のデプロイリソース (oc create deployment コマンドを使用して作成) には、以下の項目が表示されます。 

```
apiVersion: apps/v1
kind: Deployment
...output omitted...
spec:
  replicas: 1         ...(1) レプリカ数
  selector:
    matchLabels:
      app: scale      ...(2)　
  strategy: {}
  template:           ...(3)
    metadata:
      labels:
        app: scale    ...(4)
    spec:
      containers:
...output omitted...
```

### ポッドレプリカ数の手動スケーリング

開発者と管理者は、プロジェクト内のポッドレプリカの数を手動でスケーリングすることを選択できます。
予想されるトラフィックの急増に備えて多くのポッドが必要になったり、ポッド数が減少してクラスターの他の場所で使用可能なリソースを解放したりする場合があります。

ポッドのレプリカ数を増やす場合も減らす場合も、最初の手順は、スケーリングする適切なデプロイまたはデプロイ設定 (dc) を oc get コマンドを使用して特定することです。

- deployment/scaleのレプリカ数を手動で増やす
```
oc scale --replicas 5 deployment/scale
```

### ポッドの自動スケーリング

OpenShift ではアプリケーションポッドの現在の負荷に基づいて、**HorizontalPodAutoscaler(hpa) リソースタイプ**を使用してデプロイまたはデプロイ設定を自動スケーリングすることができます。 


- loadtest アプリケーションで常に 2 つのアプリケーションポッドが実行されるようにします。この数は、CPU の負荷が 50% を超えたときに最大で 10 ポッドまで増えます。
```
oc autoscale deployment/loadtest --min 2 --max 10 --cpu-percent 50
```

DeploymentなのかDeploymentConfigなのかはoc getで探せばいいだけ


- hello デプロイ設定のレプリカの数を変更し、要求された CPU 合計使用量の 80% 未満にポッドを保ちます。 
```
oc autoscale dc/hello --min 1 --max 10 --cpu-percent 80
```

- 現在のプロジェクトの水平ポッド自動スケーラーリソースに関する情報を取得する
```
[user@demo ~]$ oc get hpa
NAME   REFERENCE               TARGETS        MINPODS  MAXPODS  REPLICAS  ...
hello  DeploymentConfig/hello  <unknown>/80%  1        10       1         ...
scale  Deployment/scale        60%/80%        2        10       2         ...
```

- 特定のtargetm指定できる。
```
[user@demo ~]$ oc get hpa/loadtest
NAME      REFERENCE            TARGETS    MINPODS  MAXPODS  REPLICAS  ...
loadtest  Deployment/loadtest  172%/50%   2        10       9         ...
```


## クラスタノードへの Pod 配置を制御する

### 複数のプロジェクトへのクォータの適用

ClusterResourceQuota リソースは、永続ボリュームと同様の方法でクラスターレベルで作成され、複数のプロジェクトに適用されるリソースの制約を指定します。

クラスター管理者は、次の 2 つの方法でクラスターリソースクォータの対象となるプロジェクトを指定できる。

1. openshift.io/requester のアノテーションを使ってプロジェクトオーナーを指定する。
2. セレクターを使用する。ラベルがセレクターと一致するすべてのプロジェクトが、クォータの対象となります。 


-  (1. の例) qa ユーザーが所有する全プロジェクトのためのクラスターリソースクォータの作成例
```
oc create clusterquota user-qa(quata名) --project-annotation-selector openshift.io/requester=qa --hard pods=12,secrets=20
```

- (2. の例) environment=qa ラベルが割り当てられた全プロジェクトのためのクラスターリソースクォータの作成例
```
oc create clusterquota env-qa(quota名) --project-label-selector environment=qa --hard pods=10,services=5
```

- 削除
```
oc delete clusterquota QUOTA
```